# -*- coding: utf-8 -*-
"""sna_predict_llm.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JeWvBWmWikhlXtcgQmvb2VDXgEEKeCHA
"""

# sna_predict_llm.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import re, json, torch

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_DIR = "flan_t5_sna_adapter"
MAX_LEN = 64

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)
if DEVICE == "cuda":
    model = model.to("cuda")
gen = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=0 if DEVICE=="cuda" else -1)

def _parse_json_output(s):
    if not s: return None
    m = re.search(r"\{.*\}", s, flags=re.S)
    if m:
        try:
            return json.loads(m.group(0))
        except:
            try:
                t = m.group(0).replace("'", '"')
                t = re.sub(r",\s*}", "}", t)
                return json.loads(t)
            except:
                return None
    lower = s.lower()
    if "sensitive" in lower:
        return {"label":"SENSITIVE","score":1.0,"reason":lower[:120]}
    if "safe" in lower:
        return {"label":"SAFE","score":0.0,"reason":lower[:120]}
    return None

def predict_text_prob(text):
    prompt = (
        "Instruction:\nDecide whether the following post is appropriate for children.\n"
        "Return a JSON with keys: label ('SENSITIVE' or 'SAFE'), score (0.0-1.0), reason (short).\n\n"
        f"Input: {text}\n\nOutput:"
    )
    out = gen(prompt, max_length=MAX_LEN, do_sample=False, num_beams=2)[0]["generated_text"]
    parsed = _parse_json_output(out)
    if not parsed:
        return 0.0, 1.0
    lbl = parsed.get("label","").upper()
    score = float(parsed.get("score", 1.0 if lbl.startswith("S") else 0.0))
    score = max(0.0, min(1.0, score))
    return score, 1.0-score

def compute_node_risk(node_id, recent_post_risks, graph_feats, strike_count, alpha=1.0, beta=1.5):
    C_i = max(recent_post_risks) if recent_post_risks else 0.0
    I_i = graph_feats.get(node_id, {}).get("pagerank", 0.0)
    H_i = min(5, strike_count)
    R_i = C_i * (1 + alpha * I_i) * (1 + beta * H_i)
    return float(R_i)

from sna_predict_llm import predict_text_prob
print(predict_text_prob("This post contains sexual content and requests explicit images."))